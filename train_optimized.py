#!/usr/bin/env python
# coding: utf-8

"""
OPTIMIZED MODEL TRAINING - Conservative Improvements
Focus on proven techniques that work well for this dataset
"""

print(f"Importing required libraries...")
import numpy as np
import pandas as pd
import pickle
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.metrics import accuracy_score, roc_auc_score, classification_report, confusion_matrix
from sklearn.utils.class_weight import compute_class_weight
import warnings
warnings.filterwarnings('ignore')

print("=" * 70)
print("OPTIMIZED MODEL TRAINING")
print("=" * 70)

# Load data
print(f"\n[1/8] Loading data...")
df = pd.read_csv("data/dataset_with_realistic_hobbies.csv")
print(f"‚úì Dataset loaded: {df.shape[0]} samples")

# Pre-processing
print(f"\n[2/8] Pre-processing and smart feature engineering...")
df.columns = df.columns.str.lower().str.replace(' ', '_').str.replace('/', '_')
df.rename(columns={'nacionality':'nationality'}, inplace=True)

df['target'] = df['target'].map({'Dropout':0, 'Enrolled':1, 'Graduate':2})

# Feature engineering - only the most predictive features
print("   ‚Ä¢ Creating top predictive features...")
df['total_approved'] = df['curricular_units_1st_sem_(approved)'] + df['curricular_units_2nd_sem_(approved)']
df['total_enrolled'] = df['curricular_units_1st_sem_(enrolled)'] + df['curricular_units_2nd_sem_(enrolled)']
df['approval_rate'] = np.where(df['total_enrolled'] > 0, df['total_approved'] / df['total_enrolled'], 0)
df['avg_grade'] = (df['curricular_units_1st_sem_(grade)'] + df['curricular_units_2nd_sem_(grade)']) / 2
df['grade_trend'] = df['curricular_units_2nd_sem_(grade)'] - df['curricular_units_1st_sem_(grade)']

features = [
    'curricular_units_2nd_sem_(approved)',
    'curricular_units_2nd_sem_(grade)',
    'curricular_units_1st_sem_(approved)',
    'curricular_units_1st_sem_(grade)', 
    'tuition_fees_up_to_date',
    'scholarship_holder', 
    'age_at_enrollment', 
    'debtor', 
    'gender',
    'application_mode', 
    'curricular_units_2nd_sem_(enrolled)',
    'curricular_units_1st_sem_(enrolled)', 
    'displaced',
    'hobbies_sports', 
    'hobbies_arts', 
    'hobbies_reading',
    'hobbies_social', 
    'hobbies_gaming', 
    'hobbies_volunteering',
    'total_approved',
    'approval_rate',
    'avg_grade',
    'grade_trend',
    'target'
]

df = df[features]
print(f"‚úì Using {len(features)-1} features")

# Split
print(f"\n[3/8] Train-test split...")
df_full_train, df_test = train_test_split(df, test_size=0.2, random_state=13, stratify=df['target'])

df_full_train = df_full_train.reset_index(drop=True)
df_test = df_test.reset_index(drop=True)

y_full_train = df_full_train.target.values
y_test = df_test.target.values

df_full_train.drop('target', axis=1, inplace=True)
df_test.drop('target', axis=1, inplace=True)

print(f"‚úì Train: {len(y_full_train)}, Test: {len(y_test)}")

# Scaling
print(f"\n[4/8] Scaling features...")
scaler = StandardScaler()
df_full_train_scaled = scaler.fit_transform(df_full_train)
df_test_scaled = scaler.transform(df_test)
print("‚úì Features scaled")

# Compute enhanced class weights
print(f"\n[5/8] Computing class weights...")
class_weights = compute_class_weight('balanced', classes=np.unique(y_full_train), y=y_full_train)
# Give extra weight to minority class (Enrolled)
weight_dict = dict(zip(np.unique(y_full_train), class_weights))
weight_dict[1] = weight_dict[1] * 1.5  # Boost enrolled class
print(f"‚úì Class weights: {weight_dict}")

# Try multiple models
print(f"\n[6/8] Training and comparing models...")

# Model 1: Optimized Random Forest
print("\n   Testing Random Forest...")
rf_model = RandomForestClassifier(
    n_estimators=500,
    max_depth=25,
    min_samples_split=2,
    min_samples_leaf=1,
    max_features='sqrt',
    class_weight=weight_dict,
    random_state=13,
    n_jobs=-1
)
rf_model.fit(df_full_train_scaled, y_full_train)
y_pred_rf = rf_model.predict(df_test_scaled)
acc_rf = accuracy_score(y_test, y_pred_rf)
print(f"   Random Forest: {acc_rf*100:.2f}%")

# Model 2: Gradient Boosting
print("\n   Testing Gradient Boosting...")
gb_model = GradientBoostingClassifier(
    n_estimators=500,
    max_depth=7,
    learning_rate=0.05,
    subsample=0.8,
    min_samples_split=10,
    min_samples_leaf=5,
    random_state=13
)
gb_model.fit(df_full_train_scaled, y_full_train)
y_pred_gb = gb_model.predict(df_test_scaled)
acc_gb = accuracy_score(y_test, y_pred_gb)
print(f"   Gradient Boosting: {acc_gb*100:.2f}%")

# Model 3: Deeper Random Forest
print("\n   Testing Deeper Random Forest...")
rf_deep = RandomForestClassifier(
    n_estimators=300,
    max_depth=30,
    min_samples_split=5,
    min_samples_leaf=2,
    max_features='log2',
    class_weight=weight_dict,
    random_state=13,
    n_jobs=-1
)
rf_deep.fit(df_full_train_scaled, y_full_train)
y_pred_rf_deep = rf_deep.predict(df_test_scaled)
acc_rf_deep = accuracy_score(y_test, y_pred_rf_deep)
print(f"   Deep Random Forest: {acc_rf_deep*100:.2f}%")

# Select best
print(f"\n[7/8] Selecting best model...")
models = {
    'Random Forest (500 trees)': (rf_model, acc_rf, y_pred_rf),
    'Gradient Boosting': (gb_model, acc_gb, y_pred_gb),
    'Deep Random Forest': (rf_deep, acc_rf_deep, y_pred_rf_deep)
}

best_name = max(models, key=lambda k: models[k][1])
best_model, best_acc, y_pred_best = models[best_name]

print(f"\n{'='*70}")
print(f"BEST MODEL: {best_name}")
print(f"Test Accuracy: {best_acc*100:.2f}%")
print(f"{'='*70}")

# Detailed evaluation
print(f"\n[8/8] Final evaluation...")
y_pred_proba = best_model.predict_proba(df_test_scaled)
roc_auc = roc_auc_score(y_test, y_pred_proba, multi_class='ovr', average='macro')

print(f"\nüéØ FINAL TEST ACCURACY: {best_acc*100:.2f}%")
print(f"üìä ROC-AUC Score: {roc_auc*100:.2f}%")

print(f"\nüìã CLASSIFICATION REPORT:")
print("-" * 70)
print(classification_report(y_test, y_pred_best, target_names=['Dropout', 'Enrolled', 'Graduate'], zero_division=0))

print(f"\nüî¢ CONFUSION MATRIX:")
cm = confusion_matrix(y_test, y_pred_best)
print("\n           Predicted ‚Üí")
print("Actual ‚Üì   Dropout  Enrolled  Graduate")
print("-" * 45)
for i, name in enumerate(['Dropout ', 'Enrolled', 'Graduate']):
    print(f"{name:8}   {cm[i][0]:6}    {cm[i][1]:6}    {cm[i][2]:6}")

# Comparison
original_acc = 76.27
improvement = best_acc * 100 - original_acc
print(f"\n{'='*70}")
print(f"COMPARISON WITH ORIGINAL MODEL")
print(f"{'='*70}")
print(f"Original Model:  76.27%")
print(f"New Model:       {best_acc*100:.2f}%")
print(f"Difference:      {improvement:+.2f}%")

if best_acc >= 0.85:
    grade = "üåü EXCELLENT - 85%+ achieved!"
elif best_acc >= 0.80:
    grade = "‚≠ê VERY GOOD - Close to target!"
elif best_acc >= 0.77:
    grade = "‚úì IMPROVED - Better than original!"
elif best_acc >= 0.75:
    grade = "~ SIMILAR - Comparable performance"
else:
    grade = "‚ö†Ô∏è LOWER - Needs adjustment"

print(f"\nPerformance: {grade}")

# Save best model
print(f"\n{'='*70}")
print(f"Saving optimized model...")
output_file = "model_optimized.bin"

with open(output_file, 'wb') as f_out:
    pickle.dump((scaler, best_model, best_name), f_out)

print(f"‚úì Model saved: {output_file}")
print(f"‚úì Model type: {best_name}")
print(f"\n{'='*70}")
print(f"Training completed!")
print(f"{'='*70}")
